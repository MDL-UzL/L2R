{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torchvision\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "def gpu_usage():\n",
    "    print('gpu usage (current/max): {:.2f} / {:.2f} GB'.format(torch.cuda.memory_allocated()*1e-9, torch.cuda.max_memory_allocated()*1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from utils_nlst import AdamRegMIND,thin_plate_dense,MINDSSC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "data_path='./'#../../Learn2Reg_Dataset_release_v1.0/NLST/'\n",
    "\n",
    "#write dataloader\n",
    "\n",
    "import os\n",
    "import json\n",
    "import nibabel as nib\n",
    "from utils_voxelmorph_plusplus import *\n",
    "\n",
    "\n",
    "class NLST(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, masked=True, downsampled=False, half=False, mind=False):\n",
    "        \"\"\"\n",
    "        NLST_Dataset\n",
    "        Provides FIXED_IMG, MOVING_IMG, FIXED_KEYPOINTS, MOVING_KEYPOINTS\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(root_dir,'imagesTr')\n",
    "        self.keypoint_dir = os.path.join(root_dir,'keypointsTr')\n",
    "        self.masked = masked\n",
    "        with open(os.path.join(data_path,'NLST_dataset.json')) as f:\n",
    "            self.dataset_json = json.load(f)\n",
    "        self.shape = self.dataset_json['tensorImageShape']['0']\n",
    "        self.H, self.W, self.D = self.shape\n",
    "        self.downsampled = downsampled\n",
    "        self.half = half\n",
    "        self.mind = mind\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_json['numPairedTraining']\n",
    "\n",
    "    def get_shape(self):\n",
    "        if self.downsampled:\n",
    "            return [x//2 for x in self.shape]\n",
    "        else:\n",
    "            return self.shape\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fix_path=os.path.join(self.root_dir,self.dataset_json['training_paired_images'][idx]['fixed'])\n",
    "        mov_path=os.path.join(self.root_dir,self.dataset_json['training_paired_images'][idx]['moving'])\n",
    "    \n",
    "        fixed_img=torch.from_numpy(nib.load(fix_path).get_fdata())\n",
    "        moving_img=torch.from_numpy(nib.load(mov_path).get_fdata())\n",
    "        \n",
    "        fixed_kp=torch.from_numpy(np.genfromtxt(fix_path.replace('images','keypoints').replace('nii.gz','csv'),delimiter=','))\n",
    "        moving_kp=torch.from_numpy(np.genfromtxt(mov_path.replace('images','keypoints').replace('nii.gz','csv'),delimiter=','))\n",
    "        fixed_kp=(fixed_kp.flip(-1)/torch.tensor(self.shape))*2-1\n",
    "        moving_kp=(moving_kp.flip(-1)/torch.tensor(self.shape))*2-1\n",
    "\n",
    "        moving_mind = None\n",
    "        fixed_mind = None\n",
    "        \n",
    "        if(self.mind):\n",
    "            mask_fix = torch.from_numpy(nib.load(fix_path.replace('images', 'masks')).get_fdata()).float()\n",
    "            mask_mov = torch.from_numpy(nib.load(mov_path.replace('images', 'masks')).get_fdata()).float()\n",
    "            mind_fix_ = mask_fix*MINDSSC(fixed_img.cuda().float().unsqueeze(0).unsqueeze(0),1,2).cpu().squeeze()\n",
    "            mind_mov_ = mask_mov*MINDSSC(moving_img.cuda().float().unsqueeze(0).unsqueeze(0),1,2).cpu().squeeze()\n",
    "            fixed_mind = F.avg_pool3d(mind_fix_,2,stride=2)\n",
    "            moving_mind = F.avg_pool3d(mind_mov_,2,stride=2)\n",
    "\n",
    "\n",
    "            \n",
    "        if self.masked and not self.downsampled:\n",
    "            fixed_img=torch.from_numpy(nib.load(fix_path.replace('images', 'masks')).get_fdata())*fixed_img\n",
    "            moving_img=torch.from_numpy(nib.load(mov_path.replace('images', 'masks')).get_fdata())*moving_img\n",
    "        \n",
    "        if self.downsampled:\n",
    "            fixed_img=F.interpolate(fixed_img.view(1,1,self.H,self.W,self.D),size=(self.H//2,self.W//2,self.D//2),mode='trilinear').squeeze()\n",
    "            moving_img=F.interpolate(moving_img.view(1,1,self.H,self.W,self.D), size=(self.H//2,self.W//2,self.D//2), mode='trilinear').squeeze()\n",
    "            if self.masked:\n",
    "                fixed_img*=F.interpolate(torch.from_numpy(nib.load(fix_path.replace('images', 'masks')).get_fdata()).view(1,1,self.H,self.W,self.D),size=(self.H//2,self.W//2,self.D//2),mode='nearest').squeeze()\n",
    "                moving_img*=F.interpolate(torch.from_numpy(nib.load(mov_path.replace('images', 'masks')).get_fdata()).view(1,1,self.H,self.W,self.D),size=(self.H//2,self.W//2,self.D//2),mode='nearest').squeeze()\n",
    "\n",
    "        return fixed_img, moving_img, fixed_kp, moving_kp, fixed_mind, moving_mind\n",
    "\n",
    "NLST_dataset=NLST(data_path, downsampled=True, masked=True, half=False, mind=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "##load validation data\n",
    "\n",
    "H,W,D=NLST_dataset.get_shape()\n",
    "\n",
    "img_fix=torch.zeros((10,1,H,W,D)).float()\n",
    "img_mov=torch.zeros((10,1,H,W,D)).float()\n",
    "mind_fix=torch.zeros((10,12,H,W,D)).float()\n",
    "mind_mov=torch.zeros((10,12,H,W,D)).float()\n",
    "\n",
    "kpts_fix=[]\n",
    "kpts_mov=[]\n",
    "\n",
    "for idx,value in enumerate(range(90,100)):\n",
    "    img_fix[idx,0,...],img_mov[idx,0,...],tmp_kpts_fix, tmp_kpts_mov,mind_fix[idx,...],mind_mov[idx,...]  = NLST_dataset[value]\n",
    "    kpts_fix.append(tmp_kpts_fix)\n",
    "    kpts_mov.append(tmp_kpts_mov)\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 # Conv2d > Conv3d and 15 #BatchNorms\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_layer(model, name):\n",
    "    layer = model\n",
    "    for attr in name.split(\".\"):\n",
    "        layer = getattr(layer, attr)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def set_layer(model, name, layer):\n",
    "    try:\n",
    "        attrs, name = name.rsplit(\".\", 1)\n",
    "        model = get_layer(model, attrs)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    setattr(model, name, layer)\n",
    "import torchvision\n",
    "resnet = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "###\n",
    "resnet = torchvision.models.resnet18(pretrained=False)\n",
    "resnet.layer4 = nn.Identity()\n",
    "resnet.avgpool = nn.Identity()#nn.PixelShuffle(2)\n",
    "resnet.maxpool = nn.MaxPool3d(2)\n",
    "\n",
    "resnet.fc = nn.Sequential(nn.Unflatten(1,(8*32//2,28,24,28)),nn.Upsample(scale_factor=2,mode='trilinear'))\n",
    "#,nn.Upsample(scale_factor=2,mode='trilinear'),nn.Conv3d(32,3,3,padding=1))\n",
    "#print(resnet.conv1)\n",
    "resnet.conv1 = nn.Conv2d(2,64,5,stride=1,padding=2)\n",
    "resnet.layer2[0].conv1.stride = (1,1)\n",
    "resnet.layer2[0].downsample[0].stride=1\n",
    "\n",
    "count = 0; count2 = 0\n",
    "for name, module in resnet.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        before = get_layer(resnet, name)\n",
    "        after = nn.Conv3d(before.in_channels//2,before.out_channels//2,int(torch.tensor(before.kernel_size)[0]),stride=int(torch.tensor(before.stride).view(-1)[0]),padding=before.padding[0])\n",
    "        set_layer(resnet, name, after); count += 1\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        before = get_layer(resnet, name)\n",
    "        after = nn.BatchNorm3d(before.num_features//2)\n",
    "        set_layer(resnet, name, after); count2 += 1\n",
    "print(count,'# Conv2d > Conv3d','and',count2,'#BatchNorms')\n",
    "resnet.cuda()\n",
    "print()\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv3d(in_channels,out_channels,3,padding=1,bias=False),\\\n",
    "                                   nn.InstanceNorm3d(out_channels),nn.ReLU(inplace=True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv3d(out_channels,out_channels,1,bias=False),\\\n",
    "                                   nn.InstanceNorm3d(out_channels),nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return self.conv2(x)\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleDict({'enc1':ConvBlock(256,32),'enc2':ConvBlock(32,48),'enc3':ConvBlock(48,48),\\\n",
    "                                      'enc4':ConvBlock(48,64)})\n",
    "        self.decoder = nn.ModuleDict({'dec1':ConvBlock(64+48,48),\\\n",
    "                                      'dec2':ConvBlock(48+48,48),'dec3':ConvBlock(48+32,32)})\n",
    "        self.conv1 = ConvBlock(32,64)\n",
    "        self.conv2 = nn.Sequential(nn.Conv3d(64,32,1,bias=False),nn.InstanceNorm3d(32),nn.ReLU(inplace=True),\\\n",
    "                                 nn.Conv3d(32,32,1,bias=False),nn.InstanceNorm3d(32),nn.ReLU(inplace=True),\\\n",
    "                                 nn.Conv3d(32,3,1))\n",
    "    def forward(self, x):\n",
    "        y = []\n",
    "        upsample = nn.Upsample(scale_factor=2,mode='trilinear')\n",
    "        for i in range(4):\n",
    "            x = self.encoder['enc'+str(i+1)](x)\n",
    "            if(i<3):\n",
    "                y.append(x)\n",
    "                x = F.max_pool3d(x,2) \n",
    "        for i in range(3):\n",
    "            #if(i<3):\n",
    "            x = torch.cat((upsample(x),y.pop()),1)\n",
    "            x = self.decoder['dec'+str(i+1)](x)\n",
    "        x = self.conv1(x)\n",
    "        return upsample(self.conv2(x))\n",
    "\n",
    "unet = UNet()\n",
    "unet.cuda()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet.load_state_dict(torch.load('NLST_Example_resnet_trained.pth').state_dict())\n",
    "unet.load_state_dict(torch.load('NLST_Example_unet_trained.pth').state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/torch/nn/functional.py:4003: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5810) --> 2.4929215908050537 --> 0.6259491443634033\n",
      "tensor(5.5583) --> 4.479621887207031 --> 1.5308703184127808\n",
      "tensor(5.6216) --> 3.975072145462036 --> 1.4929213523864746\n",
      "tensor(6.2135) --> 3.493563652038574 --> 0.6372571587562561\n",
      "tensor(6.2629) --> 3.6341845989227295 --> 0.8057507276535034\n",
      "tensor(10.7615) --> 5.707571983337402 --> 1.5308440923690796\n",
      "tensor(5.8886) --> 5.357746124267578 --> 1.9684263467788696\n",
      "tensor(5.9513) --> 4.727171421051025 --> 0.7388262152671814\n",
      "tensor(11.9153) --> 7.6124982833862305 --> 4.951704025268555\n",
      "tensor(5.9239) --> 5.435888767242432 --> 1.262049913406372\n",
      "tensor([[ 5.5810,  5.5583,  5.6216,  6.2135,  6.2629, 10.7615,  5.8886,  5.9513,\n",
      "         11.9153,  5.9239],\n",
      "        [ 2.4929,  4.4796,  3.9751,  3.4936,  3.6342,  5.7076,  5.3577,  4.7272,\n",
      "          7.6125,  5.4359],\n",
      "        [ 0.6259,  1.5309,  1.4929,  0.6373,  0.8058,  1.5308,  1.9684,  0.7388,\n",
      "          4.9517,  1.2620]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t_inf = 0\n",
    "\n",
    "tre_net = torch.zeros(3,10)\n",
    "idx_test = range(91,101)\n",
    "\n",
    "out_path='outputs/NLST'\n",
    "\n",
    "\n",
    "\n",
    "resnet.eval()\n",
    "unet.eval()\n",
    "#with torch.inference_mode():\n",
    "with torch.cuda.amp.autocast():\n",
    "    for idx,val in enumerate(idx_test):\n",
    "\n",
    "\n",
    "        keypts_fix = kpts_fix[idx].cuda().float()\n",
    "        keypts_mov = kpts_mov[idx].cuda().float()\n",
    "        disp_gt = keypts_mov-keypts_fix\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.time()\n",
    "        tre_net[0,int(idx)]= disp_gt.mul(torch.tensor([H,W,D]).view(1,3).cuda()).pow(2).sum(1).sqrt().mean().item()*1.5\n",
    "        input = torch.cat((resnet(img_fix[idx:idx+1].cuda().half()),resnet(img_mov[idx:idx+1].cuda().half())),1).cuda()\n",
    "        output = unet(input)\n",
    "        pred_xyz = F.grid_sample(output,keypts_fix.cuda().half().view(1,-1,1,1,3),mode='bilinear').squeeze().t()\n",
    "\n",
    "        dense_flow = thin_plate_dense(keypts_fix.unsqueeze(0), pred_xyz.unsqueeze(0), (H*2, W*2, D*2), 4, 0.1)\n",
    "\n",
    "        disp_hr = AdamRegMIND(mind_fix[idx:idx+1].cuda().half(),mind_mov[idx:idx+1].cuda().half(),dense_flow)\n",
    "\n",
    "\n",
    "        pred_xyz2 = F.grid_sample(disp_hr,keypts_fix.cuda().view(1,-1,1,1,3),mode='bilinear').squeeze().t()\n",
    "\n",
    "\n",
    "\n",
    "        disp_field= disp_hr#F.interpolate(output,scale_factor=2,mode='trilinear')\n",
    "        disp_field=((disp_field.permute(0,2,3,4,1) /2)*(torch.tensor([224,192,224]).cuda()-1)).flip(-1).float().squeeze().cpu()\n",
    "\n",
    "        nib.save(nib.Nifti1Image(disp_field.numpy(), np.eye(4)), os.path.join(out_path, f'disp_{str(val).zfill(4)}_{str(val).zfill(4)}.nii.gz'))\n",
    "\n",
    "        t_inf += time.time()-t0\n",
    "        tre1 = (disp_gt-pred_xyz).cpu().mul(torch.tensor([H,W,D]).view(1,3)).pow(2).sum(1).sqrt()*1.5\n",
    "        tre_net[1,int(idx)] = tre1.mean().item()\n",
    "\n",
    "        tre2 = (disp_gt-pred_xyz2).cpu().mul(torch.tensor([H,W,D]).view(1,3)).pow(2).sum(1).sqrt()*1.5\n",
    "        tre_net[2,int(idx)] = tre2.mean().item()\n",
    "\n",
    "\n",
    "\n",
    "        print(tre_net[0,idx],'-->',tre1.mean().item(),'-->',tre2.mean().item())\n",
    "print(tre_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "displacement_fields= './outputs/NLST'\n",
    "data='./'\n",
    "output_dir=displacement_fields\n",
    "output_suffix='_CTCTexamp.json'\n",
    "for task in ['NLST']:\n",
    "    \n",
    "    print('Staring', task)\n",
    "    _i=os.path.join(displacement_fields)\n",
    "    _d='./'#os.path.join(data,task)\n",
    "    _o=os.path.join(output_dir,task+output_suffix)\n",
    "    _c=os.path.join('.',task+\"_90_99.json\")\n",
    "    !python evaluation.py -i {_i} -d {_d} -o{_o} -c{_c} -v\n",
    "    print(2*'\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ea9f0d30abf61677a25ad12401e4e23082d5add844bfb861bed0be870feaf59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
